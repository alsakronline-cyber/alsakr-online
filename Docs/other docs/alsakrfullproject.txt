alsakr online - AnythingLLM IDE Implementation Prompt Series
ğŸ“‹ Overview
This is a 6-phase implementation plan for building alsakr online using AnythingLLM IDE.
Each phase is a separate, executable prompt that builds on the previous one.
ğŸ¯ PHASE 1: Project Foundation & Core Infrastructure
PROJECT CONTEXT:
You are building "alsakr online" - an AI-powered online spare parts marketplace for the
MENA region. This is Phase 1: Foundation Setup.
TECHNICAL REQUIREMENTS:
- Language: Python 3.11+
- Framework: FastAPI for backend API
- Database: SQLite with proper schema design
- Project structure: Modular, production-ready architecture
YOUR TASK:
Create the complete project foundation with the following structure:
1. PROJECT DIRECTORY STRUCTURE:
alsakr-online/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app/
â”‚ â”‚ â”œâ”€â”€ __init__.py
â”‚ â”‚ â”œâ”€â”€ main.py (FastAPI app initialization)
â”‚ â”‚ â”œâ”€â”€ config.py (environment & settings)
â”‚ â”‚ â”œâ”€â”€ database.py (SQLite connection handler)
â”‚ â”‚ â””â”€â”€ models/
â”‚ â”‚ â”œâ”€â”€ __init__.py
â”‚ â”‚ â”œâ”€â”€ user.py (User model with bilingual support)
â”‚ â”‚ â”œâ”€â”€ part.py (online part catalog model)
â”‚ â”‚ â”œâ”€â”€ inquiry.py (RFQ/inquiry tracking model)
â”‚ â”‚ â””â”€â”€ vendor.py (Vendor/supplier model)
â”‚ â”œâ”€â”€ requirements.txt
â”‚ â””â”€â”€ .env.example
â”œâ”€â”€ README.md
â””â”€â”€ docker-compose.yml (optional, for future containerization)
2. DATABASE SCHEMA DESIGN:
Create SQLAlchemy models for:
a) Users table:
- id (UUID primary key)
- email, password_hash
- company_name, industry_type
- phone number
- preferred_language (en/ar)
- created_at, updated_at
b) Parts table:
- id (UUID primary key)
- part_number, manufacturer
- category, subcategory
- description_en, description_ar
- technical_specs (JSON field)
- image_url, datasheet_url
- status (active/discontinued)
- created_at, scraped_at
c) Inquiries table:
- id (UUID primary key)
- user_id (foreign key)
- part_id (foreign key)
- status (pending/quoted/closed)
- quantity, urgency_level
- notes, created_at
d) Vendors table:
- id (UUID primary key)
- company_name, contact_email
- country, response_rate
- avg_quote_time, reliability_score
3. FASTAPI APPLICATION:
Create main.py with:
- CORS middleware (for future frontend)
- Bilingual response handler (accept-language header)
- Health check endpoint: GET /api/health
- Basic auth endpoints: POST /api/auth/register, POST /api/auth/login
- Database initialization on startup
4. CONFIGURATION MANAGEMENT:
Create config.py with:
- Environment variable loading (python-decouple)
- Database URL configuration
- JWT secret key setup
- Supported languages list: ['en', 'ar']
5. REQUIREMENTS.txt:
Include these exact packages:
fastapi==0.104.1
uvicorn[standard]==0.24.0
sqlalchemy==2.0.23
python-decouple==3.8
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-multipart==0.0.6
pydantic==2.5.0
pydantic-settings==2.1.0
DELIVERABLES:
âœ… Complete folder structure with all files
âœ… Working FastAPI server that starts without errors
âœ… Database models with proper relationships
âœ… README.md with setup instructions
âœ… .env.example with all required environment variables
VALIDATION:
The server should start with: `uvicorn backend.app.main:app --reload`
Health check should respond: GET http://localhost:8000/api/health
CODE QUALITY REQUIREMENTS:
- Type hints on all functions
- Docstrings for all classes and methods
- Proper error handling (try/except blocks)
- Bilingual field naming convention (field_en, field_ar)
Generate all files with complete, production-ready code. No placeholders or TODO
comments.
ğŸ” PHASE 2: Web Scraping Engine (Scrub-Master Agent)
PROJECT CONTEXT:
You are implementing Phase 2 of alsakr online: The Scrub-Master Agent - a web scraping
system that harvests online parts data from manufacturer websites (ABB, Siemens,
Schneider Electric, SICK, Murrelektronik).
PREVIOUS PHASE:
âœ… Phase 1 completed: FastAPI backend, database models, and core infrastructure are
ready.
TECHNICAL REQUIREMENTS:
- Scraping framework: Playwright (headless browser automation)
- PDF processing: PyMuPDF (for datasheet extraction)
- Data storage: SQLite (using existing models from Phase 1)
- Rate limiting: Respect robots.txt, implement delays
YOUR TASK:
Create the complete web scraping system:
1. NEW DIRECTORY STRUCTURE:
backend/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ scrapers/
â”‚ â”‚ â”œâ”€â”€ __init__.py
â”‚ â”‚ â”œâ”€â”€ base_scraper.py (Abstract base class)
â”‚ â”‚ â”œâ”€â”€ abb_scraper.py
â”‚ â”‚ â”œâ”€â”€ siemens_scraper.py
â”‚ â”‚ â”œâ”€â”€ schneider_scraper.py
â”‚ â”‚ â”œâ”€â”€ sick_scraper.py
â”‚ â”‚ â””â”€â”€ murrelektronik_scraper.py
â”‚ â”œâ”€â”€ utils/
â”‚ â”‚ â”œâ”€â”€ __init__.py
â”‚ â”‚ â”œâ”€â”€ pdf_extractor.py (Extract text from PDF datasheets)
â”‚ â”‚ â”œâ”€â”€ image_downloader.py (Save product images)
â”‚ â”‚ â””â”€â”€ text_cleaner.py (Normalize part numbers, remove HTML)
â”‚ â””â”€â”€ api/
â”‚ â””â”€â”€ scraper_routes.py (Admin endpoints to trigger scraping)
2. BASE SCRAPER CLASS (base_scraper.py):
Create an abstract class with:
- __init__(self, brand_name, base_url, database_session)
- scrape_catalog() â†’ main orchestration method
- extract_part_details(product_url) â†’ individual part scraping
- save_to_database(part_data) â†’ insert/update parts table
- handle_rate_limiting() â†’ random delays (2-5 seconds)
- validate_part_number(part_num) â†’ check format
- Error handling with retry logic (3 attempts)
3. MANUFACTURER-SPECIFIC SCRAPERS:
For EACH manufacturer, create a scraper that inherits from BaseScraper:
ABB Scraper (abb_scraper.py):
- Target: ABB product catalog pages
- Extract: Part number, product name, category, image, datasheet PDF link
- Handle: JavaScript-loaded content using Playwright
- Special logic: ABB uses product series numbering (e.g., 1SNA)
Siemens Scraper (siemens_scraper.py):
- Target: Siemens Mall or product pages
- Extract: MLFB part number, technical specs table, images
- Handle: Multi-page catalogs with pagination
- Special logic: Siemens has "successor parts" metadata
Schneider Electric Scraper (schneider_scraper.py):
- Target: Schneider Electric product database
- Extract: Reference number, product range, datasheet
- Handle: Country-specific catalogs (detect Egypt/MENA region)
SICK Scraper (sick_scraper.py):
- Target: SICK sensor catalog
- Extract: Part number, sensor type, specifications
- Handle: Technical specification tables
Murrelektronik Scraper (murrelektronik_scraper.py):
- Target: Murrelektronik product finder
- Extract: Article number, product family, datasheet
4. PDF DATASHEET EXTRACTOR (pdf_extractor.py):
Create functions:
- download_pdf(url, save_path) â†’ download datasheet
- extract_text_from_pdf(pdf_path) â†’ use PyMuPDF to get all text
- extract_specifications_table(pdf_text) â†’ parse key-value pairs
- detect_successor_parts(pdf_text) â†’ find replacement part mentions
- Return structured JSON: {
"technical_specs": {...},
"dimensions": {...},
"successor_part": "..."
}
5. ADMIN API ENDPOINTS (scraper_routes.py):
Create FastAPI routes:
- POST /api/admin/scraper/start/{brand} â†’ Start scraping for specific brand
- GET /api/admin/scraper/status â†’ Check scraping job status
- GET /api/admin/scraper/logs â†’ View scraping logs
- POST /api/admin/scraper/stop â†’ Emergency stop
- Require admin authentication (JWT token validation)
6. SCRAPING CONFIGURATION (scrapers/config.json):
Create a JSON config file:
{
"abb": {
"base_url": "https://new.abb.com/products",
"catalog_url": "...",
"selectors": {
"product_card": ".product-item",
"part_number": ".part-number",
"datasheet_link": "a[href*='datasheet']"
},
"rate_limit_seconds": 3
},
// ... repeat for other brands
}
7. UPDATE requirements.txt:
Add these packages:
playwright==1.40.0
PyMuPDF==1.23.8
beautifulsoup4==4.12.2
aiohttp==3.9.1
fake-useragent==1.4.0
8. INITIALIZATION SCRIPT (scrapers/__init__.py):
Create a ScraperManager class:
- register_scraper(brand_name, scraper_class)
- get_scraper(brand_name) â†’ return scraper instance
- run_all_scrapers() â†’ sequential execution
- get_scraping_stats() â†’ total parts scraped per brand
DELIVERABLES:
âœ… 5 working manufacturer scrapers (ABB, Siemens, Schneider, SICK, Murrelektronik)
âœ… PDF datasheet extraction pipeline
âœ… Admin API to control scraping jobs
âœ… Error handling and logging for failed scrapes
âœ… Rate limiting to avoid IP bans
VALIDATION:
Run test scrape: POST http://localhost:8000/api/admin/scraper/start/abb
Verify: Check SQLite database for new part entries
Check: Datasheet PDFs saved in /backend/data/datasheets/
CODE QUALITY REQUIREMENTS:
- Use async/await for Playwright operations
- Implement proper exception handling (TimeoutError, NetworkError)
- Log all scraping activity to backend/logs/scraper.log
- Use Pydantic models for scraped data validation
- Comment complex CSS selectors and XPath expressions
IMPORTANT NOTES:
- Do NOT scrape all products at once (start with 50 parts per brand for testing)
- Respect robots.txt (check before implementing)
- Use rotating User-Agent headers to avoid detection
- Store raw HTML snapshots for debugging (optional)
Generate all files with complete, production-ready code. Include example usage in
README.
ğŸ¤– PHASE 3: AI Multimodal Search Engine (Vision + Chat)
PROJECT CONTEXT:
You are implementing Phase 3 of alsakr online: The AI-Powered Multimodal Search Engine.
This system allows users to find online parts using:
1. Text descriptions (English/Arabic)
2. Image uploads (nameplate photos, worn parts)
3. Voice input (Arabic/English speech-to-text)
PREVIOUS PHASES:
âœ… Phase 1: FastAPI backend + database models
âœ… Phase 2: Web scraping system with 5 manufacturer scrapers
TECHNICAL REQUIREMENTS:
- Vision AI: CLIP model (OpenAI's CLIP-ViT-B-32)
- Language AI: Sentence Transformers (multilingual-e5-large)
- Vector Database: Qdrant (for semantic search)
- Voice AI: Faster-Whisper (for speech-to-text)
- LLM: Ollama with Llama 3.2 (for conversational queries)
YOUR TASK:
Create the complete AI search system:
1. NEW DIRECTORY STRUCTURE:
backend/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ ai/
â”‚ â”‚ â”œâ”€â”€ __init__.py
â”‚ â”‚ â”œâ”€â”€ vision_agent.py (Image-based part identification)
â”‚ â”‚ â”œâ”€â”€ text_search.py (Semantic text search)
â”‚ â”‚ â”œâ”€â”€ voice_processor.py (Speech-to-text handler)
â”‚ â”‚ â”œâ”€â”€ embeddings.py (Vector embedding generation)
â”‚ â”‚ â””â”€â”€ qdrant_client.py (Vector DB operations)
â”‚ â”œâ”€â”€ api/
â”‚ â”‚ â””â”€â”€ search_routes.py (Search API endpoints)
â”‚ â””â”€â”€ models/
â”‚ â””â”€â”€ search.py (Search request/response models)
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ images/ (uploaded user images)
â”‚ â”œâ”€â”€ audio/ (uploaded voice notes)
â”‚ â””â”€â”€ embeddings/ (cached embeddings)
2. VECTOR DATABASE SETUP (qdrant_client.py):
Create QdrantManager class:
- initialize_collections() â†’ Create 2 collections:
* "parts_text" (for text embeddings, dim=1024)
* "parts_images" (for image embeddings, dim=512)
- upsert_part_embedding(part_id, embedding, metadata)
- search_by_vector(query_embedding, top_k=10)
- filter_by_brand(brand_name) â†’ Apply metadata filters
- get_part_by_id(part_id)
3. VISION AGENT (vision_agent.py):
Implement VisionAgent class:
- __init__() â†’ Load CLIP model from transformers
- identify_part_from_image(image_path) â†’ Main method:
* Preprocess image (resize, normalize)
* Extract CLIP image embedding (512-dim vector)
* Search Qdrant "parts_images" collection
* Return top 5 matching parts with similarity scores
- extract_text_from_image(image_path) â†’ Use EasyOCR:
* Extract visible text (part numbers, brand names)
* Support Arabic + English text
* Return cleaned text strings
- detect_nameplate_region(image_path) â†’ Crop to nameplate:
* Use simple object detection (optional, OpenCV)
* Focus on text-dense regions
4. TEXT SEARCH ENGINE (text_search.py):
Create TextSearchEngine class:
- __init__() â†’ Load multilingual-e5-large model
- search_by_description(query_text, language="en") â†’ Main search:
* Generate text embedding (1024-dim vector)
* Search Qdrant "parts_text" collection
* Apply language filter (match description_en or description_ar)
* Return ranked results with scores
- search_by_part_number(part_num) â†’ Exact match search:
* Normalize part number (remove spaces, uppercase)
* Query SQLite database directly
- hybrid_search(text_query, filters) â†’ Combine:
* Vector search + SQL filters (brand, category)
* Re-rank results by relevance
5. VOICE PROCESSOR (voice_processor.py):
Implement VoiceProcessor class:
- __init__() â†’ Load Faster-Whisper model ("base" size)
- transcribe_audio(audio_file_path, language="ar") â†’ Main method:
* Detect language automatically if not specified
* Convert audio to text (supports Arabic dialects)
* Return: {"text": "...", "language": "ar", "confidence": 0.95}
- process_voice_search(audio_file) â†’ Complete pipeline:
* Transcribe audio â†’ Extract text
* Pass text to TextSearchEngine.search_by_description()
* Return search results
6. EMBEDDING GENERATOR (embeddings.py):
Create EmbeddingService class:
- generate_text_embedding(text) â†’ Returns numpy array (1024-dim)
- generate_image_embedding(image_path) â†’ Returns numpy array (512-dim)
- batch_embed_parts() â†’ For initial database population:
* Read all parts from SQLite
* Generate embeddings for description_en + description_ar
* Generate embeddings from image_url
* Store in Qdrant collections
- update_single_part_embedding(part_id) â†’ For new scraped parts
7. SEARCH API ENDPOINTS (search_routes.py):
Create FastAPI routes:
POST /api/search/text
- Body: {"query": "motor bearings high temperature", "language": "en"}
- Response: List of matching parts with scores
POST /api/search/image
- Body: Multipart form with image file
- Process: Save image â†’ VisionAgent.identify_part_from_image()
- Response: Top 5 matching parts with similarity percentages
POST /api/search/voice
- Body: Multipart form with audio file (MP3, WAV)
- Process: Save audio â†’ VoiceProcessor.transcribe_audio() â†’ TextSearch
- Response: Transcription + search results
GET /api/search/part/{part_id}
- Response: Full part details + related parts
POST /api/search/advanced
- Body: Complex filters (brand, category, specs, price range)
- Process: Hybrid search (vector + SQL filters)
8. PYDANTIC MODELS (models/search.py):
Define request/response schemas:
- TextSearchRequest (query, language, filters)
- ImageSearchRequest (image file)
- VoiceSearchRequest (audio file, language)
- SearchResult (part_id, part_number, score, thumbnail)
- SearchResponse (results: List[SearchResult], query_time_ms, total_found)
9. OLLAMA INTEGRATION (Optional - Conversational Search):
Create chat_agent.py:
- query_llm(user_message, context_parts) â†’ Ask Llama 3.2:
* User: "I need a replacement for Siemens 6ES7 series PLC"
* LLM: Generates search query + suggests filters
* Call TextSearchEngine with generated query
- Bilingual system prompt (English + Arabic instructions)
10. UPDATE requirements.txt:
Add these packages:
qdrant-client==1.7.0
sentence-transformers==2.2.2
transformers==4.36.0
torch==2.1.0
pillow==10.1.0
easyocr==1.7.0
faster-whisper==0.10.0
opencv-python==4.8.1
numpy==1.24.3
11. INITIALIZATION SCRIPT:
Create backend/scripts/initialize_ai.py:
```python
# Run once to setup AI models and vector DB
# 1. Download all AI models
# 2. Create Qdrant collections
# 3. Generate embeddings for existing parts (from Phase 2 scraped data)
# 4. Verify search functionality with test queries
```
DELIVERABLES:
âœ… Working image search (upload photo â†’ get matching parts)
âœ… Working text search (English + Arabic semantic search)
âœ… Working voice search (Arabic speech â†’ text â†’ parts)
âœ… Qdrant vector database with embeddings for all scraped parts
âœ… API endpoints with proper error handling
VALIDATION TESTS:
1. Image Search Test:
- Upload photo of ABB circuit breaker nameplate
- Should return correct ABB part + similar alternatives
2. Text Search Test (English):
- Query: "24V proximity sensor IP67 rated"
- Should return SICK/Murrelektronik sensors
3. Text Search Test (Arabic):
- Query: " Ø±ÙƒØ­Ù…ÙŠØ¦Ø§Ø¨Ø±Ú¾ÙƒØ©Ø¹Ø±Ø³Ø©ÛŒÙ„Ø§Ø¹ " (high-speed electric motor)
- Should return relevant motors with Arabic descriptions
4. Voice Search Test:
- Upload Arabic voice note: " Ø«Ø­Ø¨Ø£Ù†Ø¹Ø³Ø§Ø³Ø­Ø·ØºØ¶ "
- Should transcribe correctly and return pressure sensors
5. Performance Test:
- Search response time < 2 seconds
- Qdrant search on 10,000+ parts should be under 500ms
CODE QUALITY REQUIREMENTS:
- All AI model loading should be lazy (load on first use, not at startup)
- Implement proper memory management (clear GPU cache after inference)
- Use async operations for file uploads
- Add request validation (max file size: 10MB for images, 5MB for audio)
- Implement rate limiting (max 20 searches per minute per user)
- Log all search queries to backend/logs/search_queries.log for analytics
IMPORTANT NOTES:
